{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ed3a53a-161f-4083-9ca7-6c0c111dca4e",
   "metadata": {},
   "source": [
    "## Train LSTM model \n",
    "Notebook describing general procedure to build the Phynteny model. <br> \n",
    "From here work on code to select the optimal number of memory cells, batch size, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "526784a5-3f4c-4228-8b36-27a564c705f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'format_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/local/grig0076/1175091/ipykernel_4114669/3264529860.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCuDNNLSTM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mCuDNNGRU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mformat_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'format_data'"
     ]
    }
   ],
   "source": [
    "#imports \n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, TimeDistributed, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM,CuDNNGRU\n",
    "import tensorflow as tf\n",
    "import format_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8c0c1b3-2532-457f-822e-24b0c4c1e1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to read in training genomes \n",
    "file = open(\"/home/grig0076/phispy_phrog_pickles/phrogtrainingdata_phrogs_integrasestartend_fourormore.pkl\",'rb')\n",
    "training_data = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "training_keys = list(training_data.keys())\n",
    "\n",
    "annot = pd.read_csv('/home/grig0076/LSTMs/phrog_annot_v4.tsv', sep = '\\t')\n",
    "cat_dict = dict(zip([str(i) for i in annot['phrog']], annot['category']))\n",
    "cat_dict[None] = 'unknown function'\n",
    "\n",
    "one_letter = {'DNA, RNA and nucleotide metabolism' : 4,\n",
    " 'connector' : 2,\n",
    " 'head and packaging' : 3,\n",
    " 'integration and excision': 1,\n",
    " 'lysis' : 5,\n",
    " 'moron, auxiliary metabolic gene and host takeover' : 6,\n",
    " 'other' : 7,\n",
    " 'tail' : 8,\n",
    " 'transcription regulation' : 9,\n",
    " 'unknown function' :  0 ,}\n",
    "\n",
    "#use this dictionary to generate an encoding of each phrog\n",
    "phrog_encoding = dict(zip([str(i) for i in annot['phrog']], [one_letter.get(c) for c in annot['category']]))\n",
    "\n",
    "#add a None object to this dictionary which is consist with the unknown \n",
    "phrog_encoding[None] = one_letter.get('unknown function') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "434d4840-17c9-4a1a-92cb-dcab80bb833b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'format_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/local/grig0076/1175091/ipykernel_4114669/2633068064.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_encodings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_letter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_encodings\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_functions\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'format_data' is not defined"
     ]
    }
   ],
   "source": [
    "training_encodings, features = format_data.format_data(training_data) \n",
    "\n",
    "num_functions = len(one_letter)\n",
    "max_length = np.max([len(t) for t in training_encodings])\n",
    "n_features = num_functions + len(features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835976fd-5a97-4f32-b718-c2053e32093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = 100000\n",
    "test_num = 10000\n",
    "\n",
    "#generate random indices\n",
    "idx = [i for i in range(len(training_keys))]\n",
    "random.shuffle(idx)\n",
    "\n",
    "X_train, y_train = generate_dataset([training_encodings[i] for i in idx[:train_num]], \n",
    "                                    [[f[j] for f in features] for j in idx[:train_num]],\n",
    "                                    num_functions, \n",
    "                                    n_features, \n",
    "                                    max_length)\n",
    "\n",
    "X_test, y_test = generate_dataset([training_encodings[i] for i in idx[train_num: test_num]], \n",
    "                                  [[f[j] for f in features] for j in idx[train_num: test_num]],\n",
    "                                    num_functions, \n",
    "                                    n_features, \n",
    "                                    max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f336369e-7dc1-4515-a006-76d3b96817b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conditions for the early stopping \n",
    "patience = 3\n",
    "min_delta = 0.5\n",
    "\n",
    "#conditions which can be tuned \n",
    "memory_cells = 50 \n",
    "batch_size = 128 \n",
    "epochs = 15 \n",
    "es = EarlyStopping(monitor='loss', mode='min', verbose=2, patience=patience, min_delta=min_delta)\n",
    "\n",
    "model = Sequential() \n",
    "model.add(Bidirectional(CuDNNLSTM(memory_cells, return_sequences=True),input_shape = (max_length, n_features) ))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(CuDNNLSTM(memory_cells, return_sequences = True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(CuDNNLSTM(memory_cells, return_sequences = True)))\n",
    "model.add(TimeDistributed(Dense(num_functions, activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics = ['acc']) \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c6faeb-32cf-4b59-a824-0e27d6a5932e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs = epochs, batch_size = batch_size, callbacks = [es, history], validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c779efbe-464d-41ba-b26f-5474dfb31df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the model \n",
    "correct = 0 \n",
    "for i in range(1000): \n",
    "    X, y = generate_example(training_encodings[i], [f[i] for f in features], num_functions, n_features, max_length) \n",
    "    yhat = model.predict(X) \n",
    "    \n",
    "    #the pre decoded sequences from softmax act as probabilities \n",
    "    if [one_hot_decode(i) for i in yhat] == [one_hot_decode(i) for i in y]: \n",
    "        correct += 1 \n",
    "print('Accuracy: %f' % ((correct/1000)*100.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Phynteny",
   "language": "python",
   "name": "jupyter-eg-kernel-slurm-py37-conda-1gerpr7nk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
